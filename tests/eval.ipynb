{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union:  47\n",
      "exclude:  8\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, subprocess\n",
    "\n",
    "r3_path = os.getenv('WASMR3_PATH', '/home/don/wasm-r3')\n",
    "\n",
    "def assert_cpus_disabled(start, end):\n",
    "    with open('/sys/devices/system/cpu/online', 'r') as f:\n",
    "        online_cpus = f.read().strip()\n",
    "        for cpu in range(start, end+1):\n",
    "            assert str(cpu) not in online_cpus, f\"CPU {cpu} is enabled\"\n",
    "def check_cpu_governor(start, end):\n",
    "    for cpu in range(start, end+1):\n",
    "        governor_file = f\"/sys/devices/system/cpu/cpu{cpu}/cpufreq/scaling_governor\"\n",
    "        if os.path.exists(governor_file):\n",
    "            with open(governor_file, 'r') as f:\n",
    "                governor = f.read().strip()\n",
    "                assert governor == 'performance', f\"CPU {cpu} governor is not set to performance\"\n",
    "        else:\n",
    "            print(f\"CPU {cpu} does not exist or does not have a scaling governor\")\n",
    "def assert_cover_all(expected_dirs):\n",
    "    online_tests_path = os.path.join(r3_path, 'tests/online')\n",
    "    actual_dirs = [name for name in os.listdir(online_tests_path) if os.path.isdir(os.path.join(online_tests_path, name))]\n",
    "    try:\n",
    "        assert set(actual_dirs) == set(expected_dirs)\n",
    "    except AssertionError:\n",
    "        missing_dirs = set(expected_dirs) - set(actual_dirs)\n",
    "        extra_dirs = set(actual_dirs) - set(expected_dirs)\n",
    "        print(f\"Assertion failed: Missing directories: {missing_dirs}, Extra directories: {extra_dirs}\")\n",
    "        raise\n",
    "def extract_samples_and_mean(output):\n",
    "    match = re.search(r\"recorded (\\d+) samples, mean = ([\\d\\.]+)\", output)\n",
    "    samples = int(match.group(1))\n",
    "    mean = float(match.group(2))\n",
    "    return [samples, mean]\n",
    "def extract_cycle_counts(output):\n",
    "    pattern = r\"(\\d+(?:,\\d+)*)\\s+cpu_core/cpu-cycles/\"\n",
    "    matches = re.findall(pattern, output)\n",
    "    cycle_counts = [int(match.replace(',', '')) for match in matches]\n",
    "    return cycle_counts\n",
    "def extract_summarize(output):\n",
    "    lines = output.strip().split('\\n')\n",
    "    data_line = lines[-1]\n",
    "    data_parts = data_line.split(',')\n",
    "    return [int(float(part)) for part in data_parts[1:]]\n",
    "def trace_match(metrics, testname):\n",
    "    return metrics[testname]['summary']['trace_match']\n",
    "\n",
    "test_input = \"\"\"\n",
    "DevTools listening on ws://127.0.0.1:9966/devtools/browser/f72191be-fbd6-4fbd-b21f-2703612f1f13\n",
    " Performance counter stats for 'CPU(s) 0-15':\n",
    "    40,125,664,880      cpu_core/cpu-cycles/                                                  \n",
    "       6.157604349 seconds time elapsed\n",
    " Performance counter stats for 'CPU(s) 0-15':\n",
    "     2,702,581,574      cpu_core/cpu-cycles/                                                  \n",
    "       0.267278301 seconds time elapsed\n",
    "\"\"\"\n",
    "assert extract_cycle_counts(test_input) == [40125664880, 2702581574]\n",
    "test_input_2 = \"\"\"\n",
    "================\n",
    "Run online tests\n",
    "================\n",
    "WARNING: You need a working internet connection\n",
    "WARNING: Tests depend on third party websites. If those websites changed since this testsuite was created, it might not work\n",
    "fib  -Histogram: V8.ExecuteMicroSeconds recorded 581 samples, mean = 9993.9 (flags = 0x41)\n",
    "\n",
    "581 9993.9\n",
    "nvm\n",
    "\"\"\"\n",
    "assert extract_samples_and_mean(test_input_2) == [581, 9993.9]\n",
    "test_input_3 = \"\"\"\n",
    "benchmark,instr:static_total,instr:static_replay,instrs:dynamic_total,instrs:dynamic_replay,ticks:total,ticks:replay\n",
    "/home/don/wasm-r3/tests/online/hydro/benchmark/bin_0/replay.wasm,344760,191,27138,59,228486,8934\n",
    "\"\"\"\n",
    "test_input_4 = \"\"\"\n",
    "benchmark,instr:static_total,instr:static_replay,instrs:dynamic_total,instrs:dynamic_replay,ticks:total,ticks:replay\n",
    "/home/don/wasm-r3/tests/online/multiplyDouble/benchmark/bin_0/replay.wasm,256244,238157,2.47543e+09,2100082177,9918500160,8.88911e+09\n",
    "\"\"\"\n",
    "assert(extract_summarize(test_input_3) == [344760, 191, 27138, 59, 228486, 8934])\n",
    "assert(extract_summarize(test_input_4) == [256244, 238157, 2475430000, 2100082177, 9918500160, 8889110000])\n",
    "\n",
    "# run ~/cpu.sh\n",
    "check_cpu_governor(0, 15)\n",
    "assert_cpus_disabled(16, 31)\n",
    "\n",
    "\n",
    "def get_replay_wasm(testname, opt):\n",
    "    regex = ''\n",
    "    match opt:\n",
    "        case 'noopt':\n",
    "            regex = 'merge|split|custom|benchmark'\n",
    "        case 'split':\n",
    "            regex = 'noopt|merge|custom|benchmark'\n",
    "        case 'merge':\n",
    "            regex = 'noopt|split|custom|benchmark'\n",
    "        case 'benchmark':\n",
    "            regex = 'noopt|split|merge|custom'\n",
    "        case _:\n",
    "            exit('invalid op')\n",
    "    find_command = f\"find {r3_path}/tests/online/{testname} -name replay.wasm | grep -vE '{regex}'\"\n",
    "    find_result = subprocess.run(find_command, shell=True, capture_output=True, text=True)\n",
    "    replay_path = find_result.stdout.strip()\n",
    "    return replay_path\n",
    "def get_pure_js(testname, opt):\n",
    "    regex = ''\n",
    "    match opt:\n",
    "        case 'noopt':\n",
    "            regex = 'merge|split|custom|benchmark'\n",
    "        case 'split':\n",
    "            regex = 'noopt|merge|custom|benchmark'\n",
    "        case 'merge':\n",
    "            regex = 'noopt|split|custom|benchmark'\n",
    "        case 'benchmark':\n",
    "            regex = 'noopt|split|merge|custom'\n",
    "        case _:\n",
    "            exit('invalid op')\n",
    "    find_command = f\"find {r3_path}/tests/online/{testname} -name pure.js | grep -vE '{regex}'\"\n",
    "    find_result = subprocess.run(find_command, shell=True, capture_output=True, text=True)\n",
    "    replay_path = find_result.stdout.strip()\n",
    "    return replay_path\n",
    "def get_glue_js(testname, opt):\n",
    "    regex = ''\n",
    "    match opt:\n",
    "        case 'noopt':\n",
    "            regex = 'merge|split|custom|benchmark'\n",
    "        case 'split':\n",
    "            regex = 'noopt|merge|custom|benchmark'\n",
    "        case 'merge':\n",
    "            regex = 'noopt|split|custom|benchmark'\n",
    "        case 'benchmark':\n",
    "            regex = 'noopt|split|merge|custom'\n",
    "        case _:\n",
    "            exit('invalid op')\n",
    "    find_command = f\"find {r3_path}/tests/online/{testname} -name replay.js | grep -vE '{regex}'\"\n",
    "    find_result = subprocess.run(find_command, shell=True, capture_output=True, text=True)\n",
    "    replay_path = find_result.stdout.strip()\n",
    "    return replay_path\n",
    "\n",
    "# Setup evaluation suite\n",
    "\n",
    "eval_set = ['fractals', 'parquet', 'vaporboy', 'ogv', 'factorial', 'gotemplate', 'sandspiel', 'hydro', 'hnset-bench', 'wasmsh', 'boa', 'livesplit', 'ffmpeg', 'takahirox', 'pathfinding', 'bullet', 'rustpython', 'timestretch', 'riconpacker', 'rguistyler', 'wheel', 'game-of-life', 'jsc', 'multiplyInt', 'fib', 'guiicons', 'tic-tac-toe', 'funky-kart', 'playnox', 'jqkungfu', 'figma-startpage', 'sqlpractice', 'mandelbrot', 'pacalc', 'waforth', 'roslyn', 'lichess', 'rtexpacker', 'image-convolute', 'commanderkeen', 'onnxjs', 'rguilayout', 'rfxgen', 'rtexviewer', 'multiplyDouble', 'sqlgui']\n",
    "\n",
    "skip_set = [\n",
    "    'ogv' # record run is abnormal but not filtered out by test framework because it produces something.\n",
    "]\n",
    "\n",
    "# These are excluded as they don't appear in either Made with WebAssembly(https://madewithwebassembly.com/) or Awesome-Wasm(https://github.com/mbasso/awesome-wasm)\n",
    "excluded_set = [\n",
    "    \"handy-tools\",\n",
    "    'heatmap',\n",
    "    \"kittygame\",\n",
    "    'visual6502remix',\n",
    "    'noisereduction',\n",
    "    'skeletal',\n",
    "    'uarm',\n",
    "    'virtualkc',\n",
    "]\n",
    "\n",
    "print('union: ', len(eval_set))\n",
    "print('exclude: ', len(excluded_set))\n",
    "assert_cover_all(eval_set + excluded_set)\n",
    "\n",
    "testset = eval_set\n",
    "metrics = {testname: { 'summary': {}, 'record_metrics': {}, 'replay_metrics': {}} for testname in testset }\n",
    "with open('metrics.json', 'w') as f: json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". ~/.bashrc && timeout 120s npm test -- -t fractals\n",
      ". ~/.bashrc && timeout 120s npm test -- -t takahirox\n",
      ". ~/.bashrc && timeout 120s npm test -- -t waforth\n",
      ". ~/.bashrc && timeout 120s npm test -- -t playnox\n",
      ". ~/.bashrc && timeout 120s npm test -- -t sqlpractice\n",
      ". ~/.bashrc && timeout 120s npm test -- -t hnset-bench\n",
      ". ~/.bashrc && timeout 120s npm test -- -t figma-startpage\n",
      ". ~/.bashrc && timeout 120s npm test -- -t lichess\n",
      ". ~/.bashrc && timeout 120s npm test -- -t gotemplate\n",
      ". ~/.bashrc && timeout 120s npm test -- -t onnxjs\n",
      ". ~/.bashrc && timeout 120s npm test -- -t timestretch\n",
      ". ~/.bashrc && timeout 120s npm test -- -t vaporboy\n",
      ". ~/.bashrc && timeout 120s npm test -- -t image-convolute\n",
      ". ~/.bashrc && timeout 120s npm test -- -t wasmsh\n",
      ". ~/.bashrc && timeout 120s npm test -- -t livesplit\n",
      ". ~/.bashrc && timeout 120s npm test -- -t roslyn\n",
      ". ~/.bashrc && timeout 120s npm test -- -t rustpython\n",
      ". ~/.bashrc && timeout 120s npm test -- -t boa\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/don/wasm-r3/tests/online/vim-wasm/benchmark/bin_0/stats.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     metrics[testname][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrace_match\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m isNormal\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isNormal:\n\u001b[0;32m---> 22\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr3_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tests/online/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtestname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/benchmark/bin_0/stats.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f: stats \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     23\u001b[0m         metrics[testname][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m stats\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f: json\u001b[38;5;241m.\u001b[39mdump(metrics, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/don/wasm-r3/tests/online/vim-wasm/benchmark/bin_0/stats.json'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "with open('metrics.json', 'r') as f: metrics = json.load(f)\n",
    "\n",
    "# Trace difference experiment\n",
    "timeout = 120\n",
    "\n",
    "def run_wasmr3(testname):\n",
    "    if testname in skip_set: return [testname, False]\n",
    "    command = f\". ~/.bashrc && timeout {timeout}s npm test -- -t {testname}\"\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    isNormal = result.returncode == 0\n",
    "    if not isNormal: print(result.args)\n",
    "    return [testname, isNormal]\n",
    "\n",
    "results = [run_wasmr3(testname) for testname in metrics]\n",
    "for testname, isNormal in results:\n",
    "    metrics[testname]['summary']['trace_match'] = isNormal\n",
    "    if isNormal:\n",
    "        with open(f\"{r3_path}/tests/online/{testname}/benchmark/bin_0/stats.json\", 'r') as f: stats = json.load(f)\n",
    "        metrics[testname]['summary'] |= stats\n",
    "\n",
    "with open('metrics.json', 'w') as f: json.dump(metrics, f, indent=4)\n",
    "assert get_replay_wasm('game-of-life', 'benchmark') == f\"{r3_path}/tests/online/game-of-life/benchmark/bin_0/replay.wasm\"\n",
    "assert get_glue_js('game-of-life', 'benchmark') == f\"{r3_path}/tests/online/game-of-life/benchmark/bin_0/replay.js\"\n",
    "assert get_pure_js('game-of-life', 'benchmark') == f\"{r3_path}/tests/online/game-of-life/benchmark/bin_0/pure.js\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to run:\n",
      ". ~/.bashrc && ICOUNT_FILE=/home/don/wasm-r3/tests/data/sqlpractice-icount.csv TICKS_FILE=/home/don/wasm-r3/tests/data/sqlpractice-fprofile.csv /home/don/wasm-r3-paper/oopsla/data/summarize.bash \n",
      "Failed to run:\n",
      ". ~/.bashrc && ICOUNT_FILE=/home/don/wasm-r3/tests/data/figma-startpage-icount.csv TICKS_FILE=/home/don/wasm-r3/tests/data/figma-startpage-fprofile.csv /home/don/wasm-r3-paper/oopsla/data/summarize.bash \n",
      "Failed to run:\n",
      ". ~/.bashrc && ICOUNT_FILE=/home/don/wasm-r3/tests/data/boa-icount.csv TICKS_FILE=/home/don/wasm-r3/tests/data/boa-fprofile.csv /home/don/wasm-r3-paper/oopsla/data/summarize.bash \n",
      "Failed to run:\n",
      ". ~/.bashrc && DATA_FILE=/home/don/wasm-r3/tests/data/parquet-fprofile.csv /home/don/wasm-r3-paper/oopsla/data/run-fprofile.bash /home/don/wasm-r3/tests/online/parquet/benchmark/bin_0/replay.wasm wizeng.x86-64-linux\n"
     ]
    }
   ],
   "source": [
    "import subprocess, csv, json\n",
    "\n",
    "with open('metrics.json', 'r') as f: metrics = json.load(f)\n",
    "\n",
    "# Replay characteristic experiment\n",
    "timeout = 180 # seconds\n",
    "wizard_engine_kind = ['wizeng-int']\n",
    "wizard_opt_kind = ['benchmark']\n",
    "\n",
    "# this lies as it actually collects from jit mode not int\n",
    "def run_icount(testname, engine, opt):\n",
    "    data_path = f\"/home/don/wasm-r3/tests/data/{testname}-icount.csv\"\n",
    "    replay_path = get_replay_wasm(testname, opt)\n",
    "    cmd = f'. ~/.bashrc && DATA_FILE={data_path} /home/don/wasm-r3-paper/oopsla/data/run-icount.bash {replay_path} wizeng.x86-64-linux'\n",
    "    try:        \n",
    "        result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, text=True)\n",
    "        with open(data_path, 'r') as f: \n",
    "            output = csv.DictReader(f)\n",
    "            output_dict = {row['function']: {'static': row['static'], 'dynamic': row['dynamic']} for row in output}\n",
    "            return output_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run:\")\n",
    "        print(cmd)\n",
    "        return {}\n",
    "    \n",
    "def run_fprofile(testname, engine, opt):\n",
    "    data_path = f\"/home/don/wasm-r3/tests/data/{testname}-fprofile.csv\"\n",
    "    replay_path = get_replay_wasm(testname, opt)\n",
    "    cmd = f'. ~/.bashrc && DATA_FILE={data_path} /home/don/wasm-r3-paper/oopsla/data/run-fprofile.bash {replay_path} wizeng.x86-64-linux'\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, text=True)\n",
    "        with open(data_path, 'r') as f: \n",
    "            output = csv.DictReader(f)\n",
    "            output_dict = {}\n",
    "            summary_dict = {}\n",
    "            for row in output:\n",
    "                if row['function'].startswith('r3'):\n",
    "                    output_dict[row['function']] = {'count': row['count'], 'cycles': row['cycles'], 'percent': row['percent']}\n",
    "                else:\n",
    "                    key, value = row['function'].rsplit(':', 1)\n",
    "                    summary_dict[key.strip()] = value.strip()\n",
    "            return output_dict, summary_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run:\")\n",
    "        print(cmd)\n",
    "        return {}, {}\n",
    "    \n",
    "def run_summarize(testname, engine, opt):\n",
    "    icount_path = f\"/home/don/wasm-r3/tests/data/{testname}-icount.csv\"\n",
    "    ticks_path = f\"/home/don/wasm-r3/tests/data/{testname}-fprofile.csv\"\n",
    "    replay_path = get_replay_wasm(testname, opt)\n",
    "    cmd = f'. ~/.bashrc && ICOUNT_FILE={icount_path} TICKS_FILE={ticks_path} /home/don/wasm-r3-paper/oopsla/data/summarize.bash {replay_path}'\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, text=True)\n",
    "        # instr:static_total,instr:static_replay,instrs:dynamic_total,instrs:dynamic_replay,ticks:total,ticks:replay\n",
    "        instr_static_total, instr_static_replay, instrs_dynamic_total, instr_dynamic_replay, ticks_total, ticks_replay = extract_summarize(result.stdout)\n",
    "        return {\n",
    "            'instr_static_total': instr_static_total,\n",
    "            'instr_static_replay': instr_static_replay,\n",
    "            'instrs_dynamic_total': instrs_dynamic_total,\n",
    "            'instr_dynamic_replay': instr_dynamic_replay,\n",
    "            'ticks_total': ticks_total,\n",
    "            'ticks_replay': ticks_replay,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run:\")\n",
    "        print(cmd)\n",
    "        return {}\n",
    "    \n",
    "results = []\n",
    "for testname in metrics:\n",
    "    if trace_match(metrics, testname): \n",
    "        for engine in wizard_engine_kind:\n",
    "            for opt in wizard_opt_kind:\n",
    "                if not metrics[testname]['replay_metrics'].get(engine): metrics[testname]['replay_metrics'][engine] = {}\n",
    "                if not metrics[testname]['replay_metrics'][engine].get(opt): metrics[testname]['replay_metrics'][engine][opt] = {}\n",
    "                !mkdir -p data\n",
    "                metrics[testname]['replay_metrics'][engine][opt]['icount'] = run_icount(testname, engine, opt) \n",
    "                output_dict, summary_dict = run_fprofile(testname, engine, opt) \n",
    "                metrics[testname]['replay_metrics'][engine][opt]['fprofile'] = output_dict\n",
    "                metrics[testname]['summary'] |= {**run_summarize(testname, engine, opt)}\n",
    "\n",
    "with open('metrics.json', 'w') as f: json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to run sqlpractice with noopt, engine: wizeng-int\n",
      "Failed to run sqlpractice with split, engine: wizeng-int\n",
      "Failed to run sqlpractice with merge, engine: wizeng-int\n",
      "Failed to run sqlpractice with benchmark, engine: wizeng-int\n",
      "Failed to run figma-startpage with noopt, engine: wizeng-int\n",
      "Failed to run figma-startpage with split, engine: wizeng-int\n",
      "Failed to run figma-startpage with merge, engine: wizeng-int\n",
      "Failed to run figma-startpage with benchmark, engine: wizeng-int\n",
      "Failed to run fib with noopt, engine: wizeng-int\n",
      "Failed to run fib with split, engine: wizeng-int\n",
      "Failed to run fib with merge, engine: wizeng-int\n",
      "Failed to run fib with benchmark, engine: wizeng-int\n",
      "Failed to run funky-kart with noopt, engine: wizeng-int\n",
      "Failed to run boa with noopt, engine: wizeng-int\n",
      "Failed to run boa with split, engine: wizeng-int\n",
      "Failed to run boa with merge, engine: wizeng-int\n",
      "Failed to run boa with benchmark, engine: wizeng-int\n",
      "Failed to run parquet with noopt, engine: wizeng-int\n",
      "Failed to run parquet with split, engine: wizeng-int\n",
      "Failed to run parquet with merge, engine: wizeng-int\n",
      "Failed to run parquet with benchmark, engine: wizeng-int\n"
     ]
    }
   ],
   "source": [
    "import subprocess, json, time\n",
    "\n",
    "# Replay characteristic experiment\n",
    "\n",
    "with open('metrics.json', 'r') as f: metrics = json.load(f)\n",
    "\n",
    "timeout = 180 # seconds\n",
    "engine_kind = ['sm', 'sm-base', 'sm-opt', 'v8', 'v8-liftoff', 'v8-turbofan', 'jsc', 'jsc-int','jsc-bbq','jsc-omg', 'wizeng','wizeng-int','wizeng-jit','wizeng-dyn','wasmtime','wasmer','wasmer-base']\n",
    "web_engine_kind = ['sm', 'sm-base', 'sm-opt', 'v8', 'v8-liftoff', 'v8-turbofan', 'jsc', 'jsc-int','jsc-bbq','jsc-omg']\n",
    "wizard_engine_kind = ['wizeng','wizeng-int','wizeng-jit','wizeng-dyn']\n",
    "opt_kind = ['noopt', 'split', 'merge', 'benchmark']\n",
    "web_engine_to_cmd = {\n",
    "    'sm': 'sm',\n",
    "    'sm-base': 'sm',\n",
    "    'sm-opt': 'sm',\n",
    "    'v8': 'v8',\n",
    "    'v8-liftoff': 'v8-liftoff',\n",
    "    'v8-turbofan': 'v8-turbofan',\n",
    "    'jsc': 'jsc',\n",
    "    'jsc-int': 'jsc',\n",
    "    'jsc-bbq': 'jsc',\n",
    "    'jsc-omg': 'jsc',    \n",
    "}\n",
    "\n",
    "def run_wish_you_were_fast(testname, engine, opt):\n",
    "    try:\n",
    "        global metrics\n",
    "        replay_path = get_replay_wasm(testname, opt)\n",
    "        command = f\". ~/.bashrc && RUNS=1 ENGINES={engine} timeout {timeout}s compare-engines.bash {replay_path}\"\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        if result.returncode != 0:  raise Exception\n",
    "        else:\n",
    "            runtime = float(result.stdout.split(\":\")[-1].strip())\n",
    "            metrics[testname]['replay_metrics'][engine][opt] |= { 'runtime': runtime }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run {testname} with {opt}, error: {e}\")\n",
    "        metrics[testname]['replay_metrics'][engine][opt] = {}\n",
    "\n",
    "def run_js(testname, engine, opt):\n",
    "    try:\n",
    "        global metrics\n",
    "        gluejs_path = get_glue_js(testname, opt)\n",
    "        gluejs_command = f\". ~/.bashrc && node {gluejs_path}\"\n",
    "        start_time = time.time()\n",
    "        result = subprocess.run(gluejs_command, shell=True, capture_output=True, text=True, cwd=os.path.dirname(gluejs_path))\n",
    "        end_time = time.time()\n",
    "        if result.returncode != 0: raise Exception(gluejs_command)\n",
    "        else:\n",
    "            metrics[testname]['replay_metrics'][engine][opt]['gluejs_runtime'] = end_time - start_time\n",
    "        purejs_path = get_pure_js(testname, opt)\n",
    "        purejs_command = f\". ~/.bashrc && node {purejs_path} run\"\n",
    "        start_time = time.time()\n",
    "        result = subprocess.run(purejs_command, shell=True, capture_output=True, text=True)\n",
    "        end_time = time.time()\n",
    "        if result.returncode != 0: raise Exception(purejs_command)\n",
    "        else:\n",
    "            metrics[testname]['replay_metrics'][engine][opt]['purejs_runtime'] = end_time - start_time        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run {testname} with {opt}\")\n",
    "        print(e)\n",
    "        metrics[testname]['replay_metrics'][engine][opt] = {}\n",
    "\n",
    "def run_wizard(testname, engine, opt):\n",
    "    global metrics\n",
    "    try: \n",
    "        replay_path = get_replay_wasm(testname, opt)\n",
    "        replay_size = os.path.getsize(replay_path)\n",
    "        metrics[testname]['replay_metrics'][engine][opt]['file_size'] = replay_size\n",
    "        command = f\". ~/.bashrc && timeout {timeout}s  wizeng.x86-64-linux --metrics --monitors=profile {replay_path}\"\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        if result.returncode != 0: raise Exception\n",
    "        _, profile = result.stdout.split(\"pregen:time_us\")\n",
    "        profile = 'pregen:time_us' + profile\n",
    "        # Make replay_metrics after \"pregen:time_us\" a key of some object\n",
    "        metrics[testname]['replay_metrics'][engine][opt] |= {line.rsplit(\":\", 1)[0].strip(): line.rsplit(\":\", 1)[1].strip().replace(\"μs\", \"\").strip() for line in profile.split(\"\\n\") if line}\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run {testname} with {opt}, engine: {engine}\")\n",
    "        metrics[testname]['replay_metrics'][engine][opt] = {}\n",
    "\n",
    "for testname in metrics:\n",
    "    if trace_match(metrics, testname):\n",
    "        for engine in engine_kind:\n",
    "            metrics[testname]['replay_metrics'][engine] = {}\n",
    "            for opt in opt_kind:\n",
    "                metrics[testname]['replay_metrics'][engine][opt] = {}\n",
    "        # for engine in engine_kind:\n",
    "        #     for opt in ['benchmark']:\n",
    "        #         run_wish_you_were_fast(testname, engine, opt)\n",
    "        # for engine in web_engine_kind:\n",
    "        #     for opt in ['benchmark']:\n",
    "        #         run_js(testname, engine, opt)\n",
    "        for engine in ['wizeng-int']:\n",
    "            for opt in opt_kind:\n",
    "                run_wizard(testname, engine, opt)\n",
    "\n",
    "with open('metrics.json', 'w') as f: json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trace_match' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m testname \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtrace_match\u001b[49m(metrics, testname):\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstrumented\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     36\u001b[0m             metrics[testname][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecord_metrics\u001b[39m\u001b[38;5;124m'\u001b[39m][option] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trace_match' is not defined"
     ]
    }
   ],
   "source": [
    "import subprocess, time, json, os\n",
    "\n",
    "with open('metrics.json', 'r') as f: metrics = json.load(f)\n",
    "def trace_match(metrics, testname): return metrics[testname]['summary']['trace_match']\n",
    "\n",
    "# Record overhead experiment\n",
    "timeout = 120 # seconds\n",
    "chromium_path = os.getenv('WASMR3_PATH', '/home/don/.cache/ms-playwright/chromium-1105/chrome-linux/chrome')\n",
    "perf_sh_path = os.path.join('PERFSH_PATH', '/home/don/wasm-r3/tests/perf.sh')\n",
    "CDP_PORT = os.getenv('CDP_PORT', 9997)\n",
    "os.environ['CDP_PORT'] = str(CDP_PORT)\n",
    "option_to_cmd = {\n",
    "    'original': '--noRecord',\n",
    "    'instrumented': '',\n",
    "}\n",
    "def run_command(testname, option):\n",
    "    try:\n",
    "        subprocess.run([\"killall\", \"-9\", \"chrome\"])\n",
    "        chromium_cmd = f\". ~/.bashrc && {chromium_path} --renderer-process-limit=1 --no-sandbox --remote-debugging-port={CDP_PORT} --js-flags='--slow-histograms' --renderer-cmd-prefix='bash {perf_sh_path}'\"\n",
    "        wasmr3_cmd = f\". ~/.bashrc && timeout {timeout}s npm test -- --evalRecord {option_to_cmd[option]} -t {testname}\"\n",
    "        output_path = f\"{testname}_{option}_output.txt\"\n",
    "        with open(output_path, 'w') as f: subprocess.Popen(chromium_cmd, shell=True, stdout=f , stderr=f)\n",
    "        result = subprocess.run(wasmr3_cmd, shell=True, stdout=subprocess.PIPE, text=True)\n",
    "        time.sleep(3)\n",
    "        with open(output_path, 'r') as f: output = f.read()\n",
    "        cycle_counts = extract_cycle_counts(output)\n",
    "        samples, mean = extract_samples_and_mean(result.stdout)\n",
    "        return [testname, option, cycle_counts, samples, mean]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run {testname} with {option}, error: {e}\")\n",
    "        return [testname, option, -1, -1, -1]\n",
    "    \n",
    "results = []\n",
    "for testname in metrics:\n",
    "    if trace_match(metrics, testname):\n",
    "        for option in ['original', 'instrumented']:\n",
    "            metrics[testname]['record_metrics'][option] = []\n",
    "            print(f\"Running {testname} with {option}\")\n",
    "            for i in range(10):\n",
    "                testname, _, cycles, samples, mean = run_command(testname, option) \n",
    "                metrics[testname]['record_metrics'][option].append({'samples': samples, 'mean': mean, 'cycles': cycles})\n",
    "                with open('metrics.json', 'w') as f: json.dump(metrics, f, indent=4)\n",
    "\n",
    "with open('metrics.json', 'w') as f: json.dump(metrics, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
