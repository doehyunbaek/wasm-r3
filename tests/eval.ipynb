{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yusung_set:  30\n",
      "jakob_set:  20\n",
      "doehyun_set:  6\n",
      "union:  48\n",
      "exclude:  8\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, subprocess\n",
    "\n",
    "r3_path = os.getenv('WASMR3_PATH', '/home/don/wasm-r3')\n",
    "\n",
    "def assert_cpus_disabled(start, end):\n",
    "    with open('/sys/devices/system/cpu/online', 'r') as f:\n",
    "        online_cpus = f.read().strip()\n",
    "        for cpu in range(start, end+1):\n",
    "            assert str(cpu) not in online_cpus, f\"CPU {cpu} is enabled\"\n",
    "def check_cpu_governor(start, end):\n",
    "    for cpu in range(start, end+1):\n",
    "        governor_file = f\"/sys/devices/system/cpu/cpu{cpu}/cpufreq/scaling_governor\"\n",
    "        if os.path.exists(governor_file):\n",
    "            with open(governor_file, 'r') as f:\n",
    "                governor = f.read().strip()\n",
    "                assert governor == 'performance', f\"CPU {cpu} governor is not set to performance\"\n",
    "        else:\n",
    "            print(f\"CPU {cpu} does not exist or does not have a scaling governor\")\n",
    "def assert_cover_all(expected_dirs):\n",
    "    online_tests_path = os.path.join(r3_path, 'tests/online')\n",
    "    actual_dirs = [name for name in os.listdir(online_tests_path) if os.path.isdir(os.path.join(online_tests_path, name))]\n",
    "    try:\n",
    "        assert set(actual_dirs) == set(expected_dirs)\n",
    "    except AssertionError:\n",
    "        missing_dirs = set(expected_dirs) - set(actual_dirs)\n",
    "        extra_dirs = set(actual_dirs) - set(expected_dirs)\n",
    "        print(f\"Assertion failed: Missing directories: {missing_dirs}, Extra directories: {extra_dirs}\")\n",
    "        raise\n",
    "def extract_samples_and_mean(output):\n",
    "    match = re.search(r\"recorded (\\d+) samples, mean = ([\\d\\.]+)\", output)\n",
    "    samples = int(match.group(1))\n",
    "    mean = float(match.group(2))\n",
    "    return [samples, mean]\n",
    "def extract_cycle_counts(output):\n",
    "    pattern = r\"(\\d+(?:,\\d+)*)\\s+cpu_core/cpu-cycles/\"\n",
    "    matches = re.findall(pattern, output)\n",
    "    cycle_counts = [int(match.replace(',', '')) for match in matches]\n",
    "    return cycle_counts\n",
    "def extract_summarize(output):\n",
    "    lines = output.strip().split('\\n')\n",
    "    data_line = lines[-1]\n",
    "    data_parts = data_line.split(',')\n",
    "    return [int(float(part)) for part in data_parts[1:]]\n",
    "\n",
    "test_input = \"\"\"\n",
    "DevTools listening on ws://127.0.0.1:9966/devtools/browser/f72191be-fbd6-4fbd-b21f-2703612f1f13\n",
    " Performance counter stats for 'CPU(s) 0-15':\n",
    "    40,125,664,880      cpu_core/cpu-cycles/                                                  \n",
    "       6.157604349 seconds time elapsed\n",
    " Performance counter stats for 'CPU(s) 0-15':\n",
    "     2,702,581,574      cpu_core/cpu-cycles/                                                  \n",
    "       0.267278301 seconds time elapsed\n",
    "\"\"\"\n",
    "assert extract_cycle_counts(test_input) == [40125664880, 2702581574]\n",
    "test_input_2 = \"\"\"\n",
    "================\n",
    "Run online tests\n",
    "================\n",
    "WARNING: You need a working internet connection\n",
    "WARNING: Tests depend on third party websites. If those websites changed since this testsuite was created, it might not work\n",
    "fib  -Histogram: V8.ExecuteMicroSeconds recorded 581 samples, mean = 9993.9 (flags = 0x41)\n",
    "\n",
    "581 9993.9\n",
    "nvm\n",
    "\"\"\"\n",
    "assert extract_samples_and_mean(test_input_2) == [581, 9993.9]\n",
    "test_input_3 = \"\"\"\n",
    "benchmark,instr:static_total,instr:static_replay,instrs:dynamic_total,instrs:dynamic_replay,ticks:total,ticks:replay\n",
    "/home/don/wasm-r3/tests/online/hydro/benchmark/bin_0/replay.wasm,344760,191,27138,59,228486,8934\n",
    "\"\"\"\n",
    "test_input_4 = \"\"\"\n",
    "benchmark,instr:static_total,instr:static_replay,instrs:dynamic_total,instrs:dynamic_replay,ticks:total,ticks:replay\n",
    "/home/don/wasm-r3/tests/online/multiplyDouble/benchmark/bin_0/replay.wasm,256244,238157,2.47543e+09,2100082177,9918500160,8.88911e+09\n",
    "\"\"\"\n",
    "assert(extract_summarize(test_input_3) == [344760, 191, 27138, 59, 228486, 8934])\n",
    "assert(extract_summarize(test_input_4) == [256244, 238157, 2475430000, 2100082177, 9918500160, 8889110000])\n",
    "\n",
    "# run ~/cpu.sh\n",
    "check_cpu_governor(0, 15)\n",
    "assert_cpus_disabled(16, 31)\n",
    "\n",
    "# Setup evaluation suite\n",
    "\n",
    "yusung_set = [\n",
    "    \"bullet\",\n",
    "    \"factorial\",\n",
    "    \"ffmpeg\",\n",
    "    \"fractals\",\n",
    "    \"funky-kart\",\n",
    "    \"game-of-life\",\n",
    "    \"gotemplate\",\n",
    "    \"hnset-bench\",\n",
    "    'hydro',\n",
    "    \"jqkungfu\",\n",
    "    \"lichess\",\n",
    "    \"mandelbrot\",\n",
    "    \"ogv\",\n",
    "    \"onnxjs\",\n",
    "    \"pacalc\",\n",
    "    'parquet',\n",
    "    \"playnox\",\n",
    "    \"roslyn\",\n",
    "    \"rustpython\",\n",
    "    \"sandspiel\",\n",
    "    \"sqlgui\",\n",
    "    \"sqlpractice\",\n",
    "    \"takahirox\",\n",
    "    \"tic-tac-toe\", # flaky\n",
    "    \"timestretch\",\n",
    "    \"vaporboy\",\n",
    "    \"video\",\n",
    "    \"waforth\",\n",
    "    \"wasmsh\",\n",
    "    \"wheel\",\n",
    "]\n",
    "\n",
    "jakob_set = [\n",
    "    \"boa\",\n",
    "    \"commanderkeen\",\n",
    "    \"ffmpeg\",\n",
    "    \"fib\",\n",
    "    \"figma-startpage\",\n",
    "    \"funky-kart\",\n",
    "    \"game-of-life\",\n",
    "    \"guiicons\",\n",
    "    'image-convolute',\n",
    "    \"jsc\",\n",
    "    'multiplyDouble',\n",
    "    \"multiplyInt\",\n",
    "    \"ogv\",\n",
    "    \"pathfinding\",\n",
    "    \"riconpacker\",\n",
    "    \"rtexviewer\",\n",
    "    \"sandspiel\",\n",
    "    \"sqlgui\",\n",
    "    \"takahirox\",\n",
    "    \"video\",\n",
    "]\n",
    "\n",
    "doehyun_set = [\n",
    "    'livesplit',\n",
    "    'rfxgen',\n",
    "    'rguilayout',\n",
    "    'rguistyler',\n",
    "    'rtexpacker',\n",
    "    'vim-wasm',\n",
    "]\n",
    "\n",
    "# These are excluded as they don't appear in either Made with WebAssembly(https://madewithwebassembly.com/) or Awesome-Wasm(https://github.com/mbasso/awesome-wasm)\n",
    "excluded_set = [\n",
    "    \"handy-tools\",\n",
    "    'heatmap',\n",
    "    \"kittygame\",\n",
    "    'visual6502remix',\n",
    "    'noisereduction',\n",
    "    'skeletal',\n",
    "    'uarm',\n",
    "    'virtualkc',\n",
    "]\n",
    "\n",
    "print('yusung_set: ', len(yusung_set))\n",
    "print('jakob_set: ', len(jakob_set))\n",
    "print('doehyun_set: ', len(doehyun_set))\n",
    "\n",
    "union = list(set(yusung_set) | set(jakob_set) | set(doehyun_set))\n",
    "intersection = list(set(yusung_set) & set(jakob_set))\n",
    "print('union: ', len(union))\n",
    "print('exclude: ', len(excluded_set))\n",
    "assert_cover_all(union + excluded_set)\n",
    "\n",
    "testset = union\n",
    "metrics = {testname: { 'summary': {}, 'record_metrics': {}, 'replay_metrics': {}} for testname in testset }\n",
    "with open('metrics.json', 'w') as f: json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". ~/.bashrc && timeout 120s npm test -- -t fractals\n",
      ". ~/.bashrc && timeout 120s npm test -- -t takahirox\n",
      ". ~/.bashrc && timeout 120s npm test -- -t waforth\n",
      ". ~/.bashrc && timeout 120s npm test -- -t vim-wasm\n",
      ". ~/.bashrc && timeout 120s npm test -- -t playnox\n",
      ". ~/.bashrc && timeout 120s npm test -- -t hnset-bench\n",
      ". ~/.bashrc && timeout 120s npm test -- -t lichess\n",
      ". ~/.bashrc && timeout 120s npm test -- -t gotemplate\n",
      ". ~/.bashrc && timeout 120s npm test -- -t onnxjs\n",
      ". ~/.bashrc && timeout 120s npm test -- -t timestretch\n",
      ". ~/.bashrc && timeout 120s npm test -- -t vaporboy\n",
      ". ~/.bashrc && timeout 120s npm test -- -t image-convolute\n",
      ". ~/.bashrc && timeout 120s npm test -- -t wasmsh\n",
      ". ~/.bashrc && timeout 120s npm test -- -t livesplit\n",
      ". ~/.bashrc && timeout 120s npm test -- -t roslyn\n",
      ". ~/.bashrc && timeout 120s npm test -- -t rustpython\n",
      ". ~/.bashrc && timeout 120s npm test -- -t wheel\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "with open('metrics.json', 'r') as f: metrics = json.load(f)\n",
    "\n",
    "# Trace difference experiment\n",
    "flaky = ['tic-tac-toe']\n",
    "timeout = 120\n",
    "\n",
    "def run_wasmr3(testname):\n",
    "    if testname in flaky: return [testname, False]\n",
    "    command = f\". ~/.bashrc && timeout {timeout}s npm test -- -t {testname}\"\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    isNormal = result.returncode == 0\n",
    "    if not isNormal: print(result.args)\n",
    "    return [testname, isNormal]\n",
    "\n",
    "results = [run_wasmr3(testname) for testname in metrics]\n",
    "for testname, isNormal in results:\n",
    "    metrics[testname]['summary']['trace_match'] = isNormal\n",
    "    if isNormal:\n",
    "        with open(f\"{r3_path}/tests/online/{testname}/benchmark/bin_0/stats.json\", 'r') as f: stats = json.load(f)\n",
    "        metrics[testname]['summary'] |= stats\n",
    "\n",
    "def trace_match(metrics, testname):\n",
    "    return metrics[testname]['summary']['trace_match']\n",
    "\n",
    "with open('metrics.json', 'w') as f: json.dump(metrics, f, indent=4)\n",
    "\n",
    "def get_replay_wasm(testname, opt):\n",
    "    regex = ''\n",
    "    match opt:\n",
    "        case 'noopt':\n",
    "            regex = 'merge|split|custom|benchmark'\n",
    "        case 'split':\n",
    "            regex = 'noopt|merge|custom|benchmark'\n",
    "        case 'merge':\n",
    "            regex = 'noopt|split|custom|benchmark'\n",
    "        case 'benchmark':\n",
    "            regex = 'noopt|split|merge|custom'\n",
    "        case _:\n",
    "            exit('invalid op')\n",
    "    find_command = f\"find {r3_path}/tests/online/{testname} -name replay.wasm | grep -vE '{regex}'\"\n",
    "    find_result = subprocess.run(find_command, shell=True, capture_output=True, text=True)\n",
    "    replay_path = find_result.stdout.strip()\n",
    "    return replay_path\n",
    "assert get_replay_wasm('game-of-life', 'benchmark') == f\"{r3_path}/tests/online/game-of-life/benchmark/bin_0/replay.wasm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to run:\n",
      ". ~/.bashrc && ICOUNT_FILE=/home/don/wasm-r3/tests/data/sqlpractice-icount.csv TICKS_FILE=/home/don/wasm-r3/tests/data/sqlpractice-fprofile.csv /home/don/wasm-r3-paper/oopsla/data/summarize.bash /home/don/wasm-r3/tests/online/sqlpractice/benchmark/bin_0/replay.wasm\n"
     ]
    }
   ],
   "source": [
    "import subprocess, csv, json\n",
    "\n",
    "with open('metrics.json', 'r') as f: metrics = json.load(f)\n",
    "\n",
    "# Replay characteristic experiment\n",
    "timeout = 180 # seconds\n",
    "wizard_engine_kind = ['wizeng-int']\n",
    "wizard_opt_kind = ['benchmark']\n",
    "\n",
    "# this lies as it actually collects from jit mode not int\n",
    "def run_icount(testname, engine, opt):\n",
    "    data_path = f\"/home/don/wasm-r3/tests/data/{testname}-icount.csv\"\n",
    "    replay_path = get_replay_wasm(testname, opt)\n",
    "    cmd = f'. ~/.bashrc && DATA_FILE={data_path} /home/don/wasm-r3-paper/oopsla/data/run-icount.bash {replay_path} wizeng.x86-64-linux'\n",
    "    try:        \n",
    "        result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, text=True)\n",
    "        with open(data_path, 'r') as f: \n",
    "            output = csv.DictReader(f)\n",
    "            output_dict = {row['function']: {'static': row['static'], 'dynamic': row['dynamic']} for row in output}\n",
    "            return output_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run:\")\n",
    "        print(cmd)\n",
    "        return {}\n",
    "    \n",
    "def run_fprofile(testname, engine, opt):\n",
    "    data_path = f\"/home/don/wasm-r3/tests/data/{testname}-fprofile.csv\"\n",
    "    replay_path = get_replay_wasm(testname, opt)\n",
    "    cmd = f'. ~/.bashrc && DATA_FILE={data_path} /home/don/wasm-r3-paper/oopsla/data/run-fprofile.bash {replay_path} wizeng.x86-64-linux'\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, text=True)\n",
    "        with open(data_path, 'r') as f: \n",
    "            output = csv.DictReader(f)\n",
    "            output_dict = {}\n",
    "            summary_dict = {}\n",
    "            for row in output:\n",
    "                if row['function'].startswith('r3'):\n",
    "                    output_dict[row['function']] = {'count': row['count'], 'cycles': row['cycles'], 'percent': row['percent']}\n",
    "                else:\n",
    "                    key, value = row['function'].rsplit(':', 1)\n",
    "                    summary_dict[key.strip()] = value.strip()\n",
    "            return output_dict, summary_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run:\")\n",
    "        print(cmd)\n",
    "        return {}, {}\n",
    "    \n",
    "def run_summarize(testname, engine, opt):\n",
    "    icount_path = f\"/home/don/wasm-r3/tests/data/{testname}-icount.csv\"\n",
    "    ticks_path = f\"/home/don/wasm-r3/tests/data/{testname}-fprofile.csv\"\n",
    "    replay_path = get_replay_wasm(testname, opt)\n",
    "    cmd = f'. ~/.bashrc && ICOUNT_FILE={icount_path} TICKS_FILE={ticks_path} /home/don/wasm-r3-paper/oopsla/data/summarize.bash {replay_path}'\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, text=True)\n",
    "        # instr:static_total,instr:static_replay,instrs:dynamic_total,instrs:dynamic_replay,ticks:total,ticks:replay\n",
    "        instr_static_total, instr_static_replay, instrs_dynamic_total, instr_dynamic_replay, ticks_total, ticks_replay = extract_summarize(result.stdout)\n",
    "        return {\n",
    "            'instr_static_total': instr_static_total,\n",
    "            'instr_static_replay': instr_static_replay,\n",
    "            'instrs_dynamic_total': instrs_dynamic_total,\n",
    "            'instr_dynamic_replay': instr_dynamic_replay,\n",
    "            'ticks_total': ticks_total,\n",
    "            'ticks_replay': ticks_replay,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run:\")\n",
    "        print(cmd)\n",
    "        return {}\n",
    "    \n",
    "results = []\n",
    "for testname in metrics:\n",
    "    if trace_match(metrics, testname): \n",
    "        for engine in wizard_engine_kind:\n",
    "            for opt in wizard_opt_kind:\n",
    "                if not metrics[testname]['replay_metrics'].get(engine): metrics[testname]['replay_metrics'][engine] = {}\n",
    "                if not metrics[testname]['replay_metrics'][engine].get(opt): metrics[testname]['replay_metrics'][engine][opt] = {}\n",
    "                !mkdir -p data\n",
    "                metrics[testname]['replay_metrics'][engine][opt]['icount'] = run_icount(testname, engine, opt) \n",
    "                output_dict, summary_dict = run_fprofile(testname, engine, opt) \n",
    "                metrics[testname]['replay_metrics'][engine][opt]['fprofile'] = output_dict\n",
    "                metrics[testname]['summary'] |= {**run_summarize(testname, engine, opt)}\n",
    "\n",
    "with open('metrics.json', 'w') as f: json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to run mandelbrot with benchmark, error: \n",
      "Failed to run mandelbrot with benchmark, error: \n",
      "Failed to run sqlpractice with benchmark, error: \n",
      "Failed to run sqlpractice with benchmark, error: \n",
      "Failed to run sqlpractice with benchmark, error: \n",
      "Failed to run sqlpractice with benchmark, error: \n",
      "Failed to run sqlpractice with benchmark, error: \n",
      "Failed to run sqlpractice with benchmark, error: \n",
      "Failed to run sqlpractice with benchmark, error: \n",
      "Failed to run sqlpractice with benchmark, error: \n",
      "Failed to run sqlpractice with benchmark, error: \n",
      "Failed to run sqlpractice with benchmark, error: \n",
      "Failed to run sqlpractice with benchmark, error: \n",
      ". ~/.bashrc && timeout 180s  wizeng.x86-64-linux --metrics --monitors=profile /home/don/wasm-r3/tests/online/sqlpractice/noopt/bin_0/replay.wasm\n",
      "\n",
      ". ~/.bashrc && timeout 180s  wizeng.x86-64-linux --metrics --monitors=profile /home/don/wasm-r3/tests/online/sqlpractice/split/bin_0/replay.wasm\n",
      "\n",
      ". ~/.bashrc && timeout 180s  wizeng.x86-64-linux --metrics --monitors=profile /home/don/wasm-r3/tests/online/sqlpractice/merge/bin_0/replay.wasm\n",
      "\n",
      ". ~/.bashrc && timeout 180s  wizeng.x86-64-linux --metrics --monitors=profile /home/don/wasm-r3/tests/online/sqlpractice/benchmark/bin_0/replay.wasm\n",
      "\n",
      ". ~/.bashrc && timeout 180s  wizeng.x86-64-linux --metrics --monitors=profile /home/don/wasm-r3/tests/online/fib/noopt/bin_0/replay.wasm\n",
      "\n",
      ". ~/.bashrc && timeout 180s  wizeng.x86-64-linux --metrics --monitors=profile /home/don/wasm-r3/tests/online/fib/split/bin_0/replay.wasm\n",
      "\n",
      ". ~/.bashrc && timeout 180s  wizeng.x86-64-linux --metrics --monitors=profile /home/don/wasm-r3/tests/online/fib/merge/bin_0/replay.wasm\n",
      "\n",
      ". ~/.bashrc && timeout 180s  wizeng.x86-64-linux --metrics --monitors=profile /home/don/wasm-r3/tests/online/fib/benchmark/bin_0/replay.wasm\n",
      "\n",
      ". ~/.bashrc && timeout 180s  wizeng.x86-64-linux --metrics --monitors=profile /home/don/wasm-r3/tests/online/funky-kart/noopt/bin_0/replay.wasm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess, json\n",
    "\n",
    "# Replay characteristic experiment\n",
    "\n",
    "with open('metrics.json', 'r') as f: metrics = json.load(f)\n",
    "\n",
    "timeout = 180 # seconds\n",
    "engine_kind = ['sm', 'sm-base', 'sm-opt', 'v8', 'v8-liftoff', 'v8-turbofan', 'jsc', 'jsc-int','jsc-bbq','jsc-omg', 'wizeng','wizeng-int','wizeng-jit','wizeng-dyn','wasmtime','wasmer','wasmer-base']\n",
    "wizard_engine_kind = ['wizeng','wizeng-int','wizeng-jit','wizeng-dyn']\n",
    "opt_kind = ['noopt', 'split', 'merge', 'benchmark']\n",
    "\n",
    "def run_wish_you_were_fast(testname, engine, opt):\n",
    "    try:\n",
    "        global metrics\n",
    "        replay_path = get_replay_wasm(testname, opt)\n",
    "        command = f\". ~/.bashrc && RUNS=1 ENGINES={engine} timeout {timeout}s compare-engines.bash {replay_path}\"\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        if result.returncode != 0:  raise Exception\n",
    "        else:\n",
    "            runtime = float(result.stdout.split(\":\")[-1].strip())\n",
    "            metrics[testname]['replay_metrics'][engine][opt] |= { 'runtime': runtime }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run {testname} with {opt}, error: {e}\")\n",
    "        metrics[testname]['replay_metrics'][engine][opt] = {}\n",
    "\n",
    "\n",
    "def run_wizard(testname, engine, opt):\n",
    "    global metrics\n",
    "    replay_path = get_replay_wasm(testname, opt)\n",
    "    command = f\". ~/.bashrc && timeout {timeout}s  wizeng.x86-64-linux --metrics --monitors=profile {replay_path}\"\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    isNormal = result.returncode == 0\n",
    "    if not isNormal:\n",
    "        print(result.args)\n",
    "        print(result.stderr)\n",
    "        metrics[testname]['replay_metrics'][engine][opt] = {}\n",
    "    else:\n",
    "        monitor, profile = result.stdout.split(\"pregen:time_us\")\n",
    "        profile = 'pregen:time_us' + profile\n",
    "        # Make replay_metrics after \"pregen:time_us\" a key of some object\n",
    "        metrics[testname]['replay_metrics'][engine][opt] |= {line.rsplit(\":\", 1)[0].strip(): line.rsplit(\":\", 1)[1].strip().replace(\"μs\", \"\").strip() for line in profile.split(\"\\n\") if line}\n",
    "\n",
    "for testname in metrics:\n",
    "    if trace_match(metrics, testname):\n",
    "        for engine in engine_kind:\n",
    "            metrics[testname]['replay_metrics'][engine] = {}\n",
    "            for opt in opt_kind:\n",
    "                metrics[testname]['replay_metrics'][engine][opt] = {}\n",
    "        for engine in engine_kind:\n",
    "            for opt in ['benchmark']:\n",
    "                run_wish_you_were_fast(testname, engine, opt)\n",
    "        # for engine in wizard_engine_kind:\n",
    "        for engine in ['wizeng-int']:\n",
    "            for opt in opt_kind:\n",
    "                run_wizard(testname, engine, opt)\n",
    "\n",
    "with open('metrics.json', 'w') as f: json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to run sqlpractice with benchmark, error: [Errno 2] No such file or directory: 'data/sqlpractice-fprofile.csv'\n",
      "Failed to run sqlpractice with benchmark, error: [Errno 2] No such file or directory: 'data/sqlpractice-fprofile.csv'\n",
      "Failed to run sqlpractice with benchmark, error: could not convert string to float: 'instr:static_total'\n"
     ]
    }
   ],
   "source": [
    "import subprocess, time, re, json, os\n",
    "\n",
    "with open('metrics.json', 'r') as f: metrics = json.load(f)\n",
    "\n",
    "# Record overhead experiment\n",
    "timeout = 120 # seconds\n",
    "chromium_path = os.getenv('WASMR3_PATH', '/home/don/.cache/ms-playwright/chromium-1105/chrome-linux/chrome')\n",
    "perf_sh_path = os.path.join('PERFSH_PATH', '/home/don/wasm-r3/tests/perf.sh')\n",
    "CDP_PORT = os.getenv('CDP_PORT', 9997)\n",
    "os.environ['CDP_PORT'] = str(CDP_PORT)\n",
    "option_to_cmd = {\n",
    "    'original': '--noRecord',\n",
    "    'instrumented': '',\n",
    "}\n",
    "def run_command(testname, option):\n",
    "    try:\n",
    "        subprocess.run([\"killall\", \"-9\", \"chrome\"])\n",
    "        chromium_cmd = f\". ~/.bashrc && {chromium_path} --renderer-process-limit=1 --no-sandbox --remote-debugging-port={CDP_PORT} --js-flags='--slow-histograms' --renderer-cmd-prefix='bash {perf_sh_path}'\"\n",
    "        wasmr3_cmd = f\". ~/.bashrc && timeout {timeout}s npm test -- --evalRecord {option_to_cmd[option]} -t {testname}\"\n",
    "        output_path = f\"{testname}_{option}_output.txt\"\n",
    "        with open(output_path, 'w') as f: subprocess.Popen(chromium_cmd, shell=True, stdout=f , stderr=f)\n",
    "        result = subprocess.run(wasmr3_cmd, shell=True, stdout=subprocess.PIPE, text=True)\n",
    "        time.sleep(3)\n",
    "        with open(output_path, 'r') as f: output = f.read()\n",
    "        cycle_counts = extract_cycle_counts(output)\n",
    "        samples, mean = extract_samples_and_mean(result.stdout)\n",
    "        return [testname, option, cycle_counts, samples, mean]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run {testname} with {option}, error: {e}\")\n",
    "        return [testname, option, -1, -1, -1]\n",
    "    \n",
    "results = []\n",
    "for testname in metrics:\n",
    "    if trace_match(metrics, testname):\n",
    "        for option in ['original', 'instrumented']:\n",
    "            metrics[testname]['record_metrics'][option] = []\n",
    "            for i in range(5):\n",
    "                testname, _, cycles, samples, mean = run_command(testname, option) \n",
    "                metrics[testname]['record_metrics'][option].append({'samples': samples, 'mean': mean, 'cycles': cycles})\n",
    "\n",
    "with open('metrics.json', 'w') as f: json.dump(metrics, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
